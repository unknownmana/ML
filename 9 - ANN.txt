import numpy as np
x=np.array(([2,9],[1,5],[3,6]),dtype=float)
y=np.array(([92],[86],[89]),dtype=float)
x=x/np.amax(x,axis=0)
y=y/100

# Sigmoid function
def sigmoid(x):
  return 1/(1+np.exp(-x))

# Derivative of sigmoid function
def derivatives_sigmoid(x):
  return x*(1-x)

# Variable initialization
epoch=5000
lr=0.1
inputlayer_neurons=2
hiddenlayer_neurons=3
output_neurons=1

# weight and bias initialization
wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))
bh=np.random.uniform(size=(1,hiddenlayer_neurons))
wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))
bout=np.random.uniform(size=(1,output_neurons))

# draws a random range Of numbers Of dim x*y
for i in range(epoch):
  # Forward Propogation
  hinp1=np.dot(x,wh)
  hinp=hinp1+bh
  hlayer_act=sigmoid(hinp)
  outinp1=np.dot(hlayer_act,wout)
  outinp=outinp1+bout
  output=sigmoid(outinp)
  # Backward Propogation
  EO=y-output
  outgrad=derivatives_sigmoid(output)
  d_output=EO*outgrad
  EH=d_output.dot(wout.T)
  # how much hidden layer wts contributed to error
  hiddengrad=derivatives_sigmoid(hlayer_act)
  d_hiddenlayer=EH*hiddengrad
  # dot product Of next layererror and current layerop
  wout+=hlayer_act.T.dot(d_output)*lr
  wh+=x.T.dot(d_hiddenlayer)*lr

print("Input: \n"+str(x))
print("Actual Output: \n"+str(y))
print("Predicted Output: \n",output)